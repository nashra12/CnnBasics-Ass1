{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1c69b3-e94e-4796-8900-1be5d37294d7",
   "metadata": {},
   "source": [
    "TOPIC: Understanding Pooling and Padding in CNN\n",
    "Q1-Describe the purpose and benefits of pooling in CNN.\n",
    "Ans->  purpose:\n",
    "\n",
    "\n",
    "Pooling is a crucial operation in Convolutional Neural Networks (CNNs) used primarily to reduce the spatial dimensions (height and width) of the input volume.\n",
    "1.Dimensionality Reduction\n",
    "2.Control Overfitting: By reducing the complexity of the model (fewer parameters), pooling can help prevent overfitting\n",
    "3.Focus on Dominant Features: Pooling helps in focusing on the most prominent features by retaining the maximum or average value from a region, which helps in preserving essential information while discarding irrelevant details.\n",
    "\n",
    "\n",
    "Types of Pooling\n",
    "\n",
    "    Max Pooling:\n",
    "    Max pooling selects the maximum value from each pooling window (region). \n",
    "\n",
    "    Average Pooling:\n",
    "    Average pooling computes the average value of the elements in each pooling window. \n",
    "\n",
    "    Global Pooling:\n",
    "    Global pooling (Global Max Pooling or Global Average Pooling) reduces each feature map to a single value. \n",
    "    \n",
    "Benefits of Pooling:\n",
    "\n",
    "1.Reduced Computational Complexity:\n",
    "By reducing the spatial dimensions, pooling decreases the number of neurons in subsequent layers, leading to fewer computations and faster training times.\n",
    "\n",
    "2.Enhanced Feature Detection:\n",
    "Pooling layers help in capturing the most relevant features from the input, which improves the network's ability to generalize and detect important patterns.\n",
    "\n",
    "\n",
    "\n",
    "Q2. Explain the difference between Min pooling and Max pooling?\n",
    "Ans-> Max Pooling\n",
    "\n",
    "Definition:\n",
    "Max pooling selects the maximum value from each pooling window (region) of the input feature map.\n",
    "\n",
    "Operation:\n",
    "In a given window (e.g., 2x2 or 3x3), max pooling picks the highest value.\n",
    "\n",
    "\n",
    "Min Pooling\n",
    "\n",
    "Definition:\n",
    "Min pooling selects the minimum value from each pooling window (region) of the input feature map.\n",
    "\n",
    "Operation:\n",
    "In a given window (e.g., 2x2 or 3x3), min pooling picks the lowest value.\n",
    "\n",
    "Max Pooling: Aims to capture the most significant features.\n",
    "Min Pooling: Focuses on the least significant features, which can be useful for specific applications that require attention to lower-intensity patterns.\n",
    "\n",
    "\n",
    "Max Pooling: Emphasizes and retains the highest values, making the network sensitive to strong features.\n",
    "Min Pooling: Emphasizes and retains the lowest values, which may help in identifying patterns or reducing noise.\n",
    "\n",
    "\n",
    "Q3. Discuss the concept of padding in CNN and its significance.\n",
    "Ans-> Padding in Convolutional Neural Networks (CNNs) refers to the process of adding extra pixels around the border of the input feature map.\n",
    "Types of Padding\n",
    "\n",
    "Valid Padding (No Padding):\n",
    "        No extra pixels are added around the border.\n",
    "        The output feature map is smaller than the input feature map.\n",
    "    Same Padding (Zero Padding):\n",
    "\n",
    "Extra pixels (usually zeros) are added around the border so that the output feature map has the same spatial dimensions as the input feature map.\n",
    "    Padding size p is calculated to maintain the same dimension.\n",
    "\n",
    "\n",
    "Q4.Compare and contrast zero-padding and valid-padding in texts of their effects on the output\n",
    "feature map size.\n",
    "Ans-> \n",
    "Zero-padding is often used to keep the spatial dimensions of the input feature map unchanged after convolution. For example, if you have a 5x5 input feature map and you use a 3x3 filter with stride 1, applying same (zero) padding ensures that the output feature map remains 5x5.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TOPIC: Exploring LeNet\n",
    "\n",
    "Q1. Provide a berief overview of LeNet-5 architechture.\n",
    "Ans->Is a very efficient convolutional neural network for handwritten character recognition.\n",
    "LeNet5 is a small network, it contains the basic modules of deep learning: convolutional layer, pooling layer, and full link layer. It is the basis of other deep learning models.\n",
    "LeNet-5 Total seven layer \n",
    "The first is the data INPUT layer. The size of the input image is uniformly normalized to 32 * 32.\n",
    "Layer 1: Convolutional Layer (C1)\n",
    "\n",
    "    Number of filters: 6\n",
    "    Filter size: 5x5\n",
    "    Stride: 1\n",
    "    Padding: 0 (valid padding)\n",
    "    Output size: 28x28x6\n",
    "    Activation function: Sigmoid (originally used, but often replaced by ReLU in modern implementations)\n",
    "\n",
    "Layer 2: Pooling/Subsampling Layer (S2)\n",
    "\n",
    "    Type: Average pooling (subsampling)\n",
    "    Pool size: 2x2\n",
    "    Stride: 2\n",
    "    Output size: 14x14x6\n",
    "\n",
    "Layer 3: Convolutional Layer (C3)\n",
    "\n",
    "    Number of filters: 16\n",
    "    Filter size: 5x5\n",
    "    Stride: 1\n",
    "    Padding: 0 (valid padding)\n",
    "    Output size: 10x10x16\n",
    "    Activation function: Sigmoid\n",
    "\n",
    "Layer 4: Pooling/Subsampling Layer (S4)\n",
    "\n",
    "    Type: Average pooling (subsampling)\n",
    "    Pool size: 2x2\n",
    "    Stride: 2\n",
    "    Output size: 5x5x16\n",
    "\n",
    "Layer 5: Convolutional Layer (C5)\n",
    "\n",
    "    Number of filters: 120\n",
    "    Filter size: 5x5\n",
    "    Stride: 1\n",
    "    Padding: 0 (valid padding)\n",
    "    Output size: 1x1x120 (flattened to 120)\n",
    "    Activation function: Sigmoid\n",
    "\n",
    "Layer 6: Fully Connected Layer (F6)\n",
    "\n",
    "    Number of neurons: 84\n",
    "    Activation function: Sigmoid\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "    Number of neurons: 10 (one for each digit class 0-9)\n",
    "    Activation function: Softmax (for classification)\n",
    "\n",
    "\n",
    "Convolutional neural networks can make good use of the structural information of images.\n",
    "Q2. Describe the key components of LeNet-5 and their respective purposes.\n",
    "Ans-> Input Layer:\n",
    "\n",
    "    Receives the input image.\n",
    "\n",
    "Convolutional Layers (C1, C3, C5):\n",
    "\n",
    "    Detect local and complex features from the input image and intermediate feature maps.\n",
    "\n",
    "Pooling Layers (S2, S4):\n",
    "\n",
    "    Reduce spatial dimensions and computational complexity while providing translation invariance.\n",
    "\n",
    "Fully Connected Layer (F6):\n",
    "\n",
    "    Combines extracted features to classify the input.\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "    Provides the final classification result.\n",
    "    \n",
    "Q3.Discuss the advantages and limitations of LeNet-5 in the context of image classification task.\n",
    "Ans-> LeNet-5 introduced the fundamental principles of CNNs, including convolutional layers, pooling layers, and fully connected layers, which are still used in modern architectures.\n",
    "\n",
    "LeNet-5 has a relatively shallow architecture (only three convolutional layers). Modern CNNs, like VGG, ResNet, and Inception, are much deeper, allowing them to learn more complex and abstract features from images.\n",
    "\n",
    "LeNet-5 was designed for 32x32 pixel images. This small input size limits its applicability to larger, more complex images commonly found in modern datasets like ImageNet.\n",
    "\n",
    "The original LeNet-5 uses sigmoid activation functions, which are less effective than modern alternatives like ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5693a-8f11-498c-ae09-17d435365038",
   "metadata": {},
   "source": [
    "Q4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTocch) and train it on a publically available dataset (e.g., MNIST). Evaluate its performance and provide\n",
    "insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a988c4ff-83f3-45c1-addd-a72c4991bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.28.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98d4dee-7df4-4c6d-b4fd-ccc9fd83423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 13:27:56.015264: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-18 13:27:56.329603: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-18 13:27:57.548589: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-18 13:27:59.583177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense,Flatten,Conv2D,MaxPooling2D,AveragePooling2D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6501eb7-e352-4a8f-aad1-799e5a50204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test)=keras.datasets.cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8832f90-97d3-434f-8ce2-9d271bd0b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train/255.0\n",
    "X_test=X_test/255.0\n",
    "y_train=keras.utils.to_categorical(y_train,10)\n",
    "y_test=keras.utils.to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ae744-60d2-4366-a2cc-5c958051abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m2,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │        \u001b[38;5;34m48,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │        \u001b[38;5;34m10,164\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,006</span> (242.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m62,006\u001b[0m (242.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,006</span> (242.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m62,006\u001b[0m (242.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2950 - loss: 1.9556"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='tanh', input_shape = (32,32,3)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "\n",
    "model.add(Conv2D(16,kernel_size=(5,5),padding='valid',activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.metrics.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,batch_size=128,epochs=2,verbose=1,validation_data=(X_test,y_test))\n",
    "score=model.evaluate(X_test,y_test)\n",
    "print(\"Test Loss:\",score[0])\n",
    "print(\"Test accuracy:\",score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d3a34-a2dc-4876-9da5-1fd5bd0cf2da",
   "metadata": {},
   "source": [
    "# TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7468d5b-96e4-4714-ac07-96b2a62b4e44",
   "metadata": {},
   "source": [
    "Q 1.Present an overview of the AlexNet architechture.\n",
    "Ans->AlexNet consists of eight layers: five convolutional layers followed by three fully connected layers. It employs several innovative techniques to improve performance and manage computational complexity.\n",
    "\n",
    "\n",
    "Why does AlexNet achieve better results?\n",
    "\n",
    "1.Relu activation function is used\n",
    "Relu function: f (x) = max (0, x)\n",
    "ReLU-based deep convolutional networks are trained several times faster than tanh and sigmoid- based networks.\n",
    "\n",
    "2.Standardization ( Local Response Normalization )\n",
    "\n",
    "After using ReLU f (x) = max (0, x), you will find that the value after the activation function has no range like the tanh and sigmoid functions, so a normalization will usually be done after ReLU.\n",
    "\n",
    "Q2.Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough\n",
    "performance.\n",
    "Ans-> AlexNet introduced several key architectural innovations that significantly contributed to its breakthrough performance in image classification tasks, particularly in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012.\n",
    "\n",
    "AlexNet replaced traditional activation functions like sigmoid and tanh with Rectified Linear Units (ReLU).\n",
    "\n",
    "ReLU activation is computationally simpler and more efficient compared to sigmoid or tanh, as it involves only a thresholding operation\n",
    "\n",
    "ReLU helps in faster convergence during training due to its non-saturating property, which addresses the vanishing gradient problem.\n",
    "\n",
    "AlexNet was trained using two GPUs.\n",
    "\n",
    "AlexNet utilized a deeper and wider architecture compared to earlier networks, with five convolutional layers and three fully connected layers.\n",
    "\n",
    "Q3.Discuss the Role of convolutional layers, pooling layers, and fully connected layers in AlexNet.\n",
    "Ans-> 1.Convolutional layers in AlexNet are responsible for extracting spatial hierarchies of features from the input images.\n",
    "\n",
    "Conv1: The first convolutional layer in AlexNet applies 96 filters of size 11x11 with a stride of 4. This initial layer captures low-level features such as edges and textures.\n",
    "Conv2: Following the first pooling layer, Conv2 uses 256 filters of size 5x5 with a stride of 1 and padding of 2. It further refines the features extracted by Conv1.\n",
    "Conv3, Conv4, Conv5: These layers continue to extract increasingly complex features. They employ smaller filter sizes (3x3) with strides of 1 and padding of 1 to maintain spatial resolution and capture more detailed patterns.\n",
    "\n",
    "2.Pooling layers in AlexNet downsample the feature maps generated by convolutional layers, reducing spatial dimensions while preserving important features.\n",
    "\n",
    "Details:\n",
    "\n",
    "    Pool1, Pool2, Pool3: These layers use max pooling with 3x3 windows and a stride of 2 in Pool1 and Pool2, and 3x3 windows with a stride of 2 in Pool3. They progressively reduce the spatial size of the feature maps.\n",
    "    \n",
    "3.Fully connected layers in AlexNet perform classification based on the features extracted by convolutional and pooling layers.\n",
    "\n",
    "Details:\n",
    "\n",
    "    FC6, FC7: These layers consist of 4096 neurons each and are fully connected to the preceding layer’s outputs. They receive flattened feature maps from the last convolutional or pooling layer.\n",
    "    FC8: The final fully connected layer with 1000 neurons corresponds to the number of classes in the ImageNet dataset and uses softmax activation for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237dbc9-2b6b-4bc0-9a05-2cf8dd5dbca4",
   "metadata": {},
   "source": [
    " # Q4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance\n",
    "# on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2933cf-218c-4db8-bcde-abdffe803e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "# Get Data\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x, y = oxflower17.load_data()\n",
    "\n",
    "x_train = x.astype('float32') / 255.0\n",
    "y_train = to_categorical(y, num_classes=17)\n",
    "\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
